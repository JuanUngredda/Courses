{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9252e6",
   "metadata": {
    "id": "ac9252e6"
   },
   "source": [
    "\n",
    "# PyTorch Fundamentals (Torch Tutorial Style): **Tensors** + **Autograd** + MLP on Nonlinear Data\n",
    "\n",
    "This notebook mirrors the structure of the **official PyTorch “Basics” tutorials** and then applies the concepts in a small end‑to‑end example:\n",
    "1. **Tensors**\n",
    "2. **Automatic Differentiation** with `torch.autograd`\n",
    "3. MLP classifier on **non-linearly separable** synthetic data (two moons)\n",
    "\n",
    "> Goal: make you fluent in the *core* PyTorch mechanics you’ll reuse later for larger models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368796",
   "metadata": {
    "id": "55368796"
   },
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579dba9",
   "metadata": {
    "id": "1579dba9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "assert torch.cuda.is_available(), \"⚠️ Please enable GPU in Runtime -> Change runtime type\"\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8cb6e",
   "metadata": {
    "id": "9fd8cb6e"
   },
   "source": [
    "\n",
    "## 1) Tensors\n",
    "\n",
    "### Informal idea\n",
    "A **tensor** is PyTorch’s core data structure: a typed, shape-aware, device-aware multi-dimensional array.\n",
    "Most deep learning is just tensor ops + automatic differentiation.\n",
    "\n",
    "### What you should walk away with\n",
    "- how to **create** tensors\n",
    "- how to inspect **shape / dtype / device**\n",
    "- basic **indexing / slicing**\n",
    "- **operations** and in-place semantics\n",
    "- moving data across **CPU/GPU**\n",
    "- interoperability with **NumPy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a7159",
   "metadata": {
    "id": "1c5a7159"
   },
   "source": [
    "### 1.1 Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fefad2",
   "metadata": {
    "id": "e7fefad2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# From Python data\n",
    "a = [[1, 2], [3, 4]]\n",
    "t1 = torch.tensor(a)\n",
    "print(\"t1:\", t1, \"| dtype:\", t1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960rut1A_KiW",
   "metadata": {
    "id": "960rut1A_KiW"
   },
   "outputs": [],
   "source": [
    "# From another tensor (shares metadata rules)\n",
    "t2 = torch.ones_like(t1)\n",
    "t3 = torch.rand_like(t1, dtype=torch.float32)\n",
    "print(\"t2:\", t2, \"| t3:\", t3, \"| t3 dtype:\", t3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YpnAusrL_LEF",
   "metadata": {
    "id": "YpnAusrL_LEF"
   },
   "outputs": [],
   "source": [
    "# Random / constant tensors\n",
    "t4 = torch.randn(3, 4)          # standard normal\n",
    "t5 = torch.zeros(2, 3, 5)       # zeros\n",
    "t6 = torch.arange(0, 10)        # 0..9\n",
    "print(\"t4 shape:\", t4.shape, \"t5 shape:\", t5.shape, \"t6:\", t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13ed19",
   "metadata": {
    "id": "6a13ed19"
   },
   "source": [
    "### 1.2 Tensor attributes: shape, dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afad74",
   "metadata": {
    "id": "d0afad74"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(2, 3, device=device, dtype=torch.float32)\n",
    "print(\"shape:\", x.shape)\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"device:\", x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d694b29",
   "metadata": {
    "id": "7d694b29"
   },
   "source": [
    "### 1.3 Indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473e142",
   "metadata": {
    "id": "3473e142"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.arange(0, 12).reshape(3, 4)\n",
    "print(x)\n",
    "\n",
    "print(\"x[0]:\", x[0])\n",
    "print(\"x[:, 1:3]:\", x[:, 1:3])\n",
    "print(\"last column:\", x[:, -1])\n",
    "\n",
    "# boolean masking\n",
    "mask = (x % 2 == 0)\n",
    "print(\"mask:\", mask)\n",
    "print(\"even elements:\", x[mask])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcafa48",
   "metadata": {
    "id": "bfcafa48"
   },
   "source": [
    "### 1.4 Basic operations (and broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a4e3",
   "metadata": {
    "id": "9706a4e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "a = torch.randn(2, 3, device=device)\n",
    "b = torch.randn(2, 3, device=device)\n",
    "\n",
    "print(\"add:\", (a + b).shape)\n",
    "print(\"mul:\", (a * b).shape)\n",
    "\n",
    "# matrix multiply\n",
    "m1 = torch.randn(2, 4, device=device)\n",
    "m2 = torch.randn(4, 3, device=device)\n",
    "mm = m1 @ m2\n",
    "print(\"matmul shape:\", mm.shape)\n",
    "\n",
    "# broadcasting: (2,3) + (3,) -> (2,3)\n",
    "v = torch.randn(3, device=device)\n",
    "c = a + v\n",
    "print(\"broadcasted add shape:\", c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad1ffe",
   "metadata": {
    "id": "64ad1ffe"
   },
   "source": [
    "### 1.5 In-place operations (use with care)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca66f6",
   "metadata": {
    "id": "f0ca66f6"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.ones(3, device=device)\n",
    "print(\"before:\", x)\n",
    "\n",
    "# in-place add\n",
    "x.add_(2)\n",
    "print(\"after add_:\", x)\n",
    "\n",
    "# In-place ops can break autograd in some situations; avoid them unless you know why you need them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e41113",
   "metadata": {
    "id": "f7e41113"
   },
   "source": [
    "### 1.6 Moving tensors across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b325c",
   "metadata": {
    "id": "799b325c"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_cpu = torch.randn(2, 2, device=\"cpu\")\n",
    "x_dev = x_cpu.to(device)\n",
    "print(\"cpu device:\", x_cpu.device, \"| moved device:\", x_dev.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc38c81",
   "metadata": {
    "id": "6dc38c81"
   },
   "source": [
    "### 1.7 NumPy bridge (CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198069c",
   "metadata": {
    "id": "2198069c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "x = torch.randn(3, 3)          # on CPU\n",
    "x_np = x.numpy()               # shares memory\n",
    "x_np[0, 0] = 123.0\n",
    "print(\"torch sees numpy change:\", x[0,0].item())\n",
    "\n",
    "y_np = np.ones((2,2), dtype=np.float32)\n",
    "y = torch.from_numpy(y_np)     # shares memory\n",
    "y[0,0] = 7.0\n",
    "print(\"numpy sees torch change:\", y_np[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04d9d3",
   "metadata": {
    "id": "be04d9d3"
   },
   "source": [
    "\n",
    "## 2) Automatic Differentiation with `torch.autograd`\n",
    "\n",
    "### Informal idea\n",
    "PyTorch can automatically compute gradients for tensor expressions by building a **dynamic computation graph** during the forward pass.\n",
    "\n",
    "### Formal idea (very compact)\n",
    "For a scalar objective $L(\\theta)$, PyTorch computes $\n",
    "\\nabla_\\theta L$ via automatic differentiation (backprop).\n",
    "\n",
    "### What you should walk away with\n",
    "- how `requires_grad` controls tracking\n",
    "- what `.backward()` does\n",
    "- how gradients accumulate in `.grad`\n",
    "- when to use `detach()` and `torch.no_grad()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1c2b",
   "metadata": {
    "id": "9cbe1c2b"
   },
   "source": [
    "### 2.1 A tiny scalar example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41dd84",
   "metadata": {
    "id": "db41dd84"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(2.0, device=device, requires_grad=True)\n",
    "y = x**3 + 2*x + 1  # y = x^3 + 2x + 1\n",
    "y.backward()        # dy/dx\n",
    "\n",
    "print(\"x:\", x.item())\n",
    "print(\"y:\", y.item())\n",
    "print(\"dy/dx should be 3x^2 + 2 =\", 3*(2.0**2) + 2)\n",
    "print(\"autograd dy/dx:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1db793",
   "metadata": {
    "id": "4d1db793"
   },
   "source": [
    "### 2.2 Vector gradients: scalar output required for `.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb711bc",
   "metadata": {
    "id": "6eb711bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(3, device=device, requires_grad=True)\n",
    "y = (x * x).sum()  # scalar\n",
    "y.backward()\n",
    "print(\"x:\", x)\n",
    "print(\"grad (should be 2x):\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea988ea",
   "metadata": {
    "id": "9ea988ea"
   },
   "source": [
    "### 2.3 Gradient accumulation (why you zero grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c412b",
   "metadata": {
    "id": "ff5c412b"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "y1 = x * 3\n",
    "y1.backward()\n",
    "print(\"after first backward:\", x.grad.item())  # 3\n",
    "\n",
    "y2 = x * 4\n",
    "y2.backward()\n",
    "print(\"after second backward (accumulated):\", x.grad.item())  # 3 + 4\n",
    "\n",
    "# reset\n",
    "x.grad.zero_()\n",
    "print(\"after zero_:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af127a90",
   "metadata": {
    "id": "af127a90"
   },
   "source": [
    "### 2.4 Detaching and `no_grad` (stop tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c91de4",
   "metadata": {
    "id": "70c91de4"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(2, 2, device=device, requires_grad=True)\n",
    "y = (x @ x).sum()\n",
    "\n",
    "# detach: new tensor shares storage but has no grad history\n",
    "x_det = x.detach()\n",
    "print(\"x_det requires_grad?\", x_det.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x * 10\n",
    "print(\"z requires_grad?\", z.requires_grad)\n",
    "\n",
    "# You typically use no_grad during evaluation/inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feced132",
   "metadata": {
    "id": "feced132"
   },
   "source": [
    "### 2.5 Inspecting the computation graph (light touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010af66",
   "metadata": {
    "id": "9010af66"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(3, device=device, requires_grad=True)\n",
    "y = (x.sin() * x).sum()\n",
    "print(\"y.grad_fn:\", y.grad_fn)  # shows last operation node\n",
    "y.backward()\n",
    "print(\"x.grad:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51001a03",
   "metadata": {},
   "source": [
    "# Gradient Descent Explained Through This PyTorch Code\n",
    "\n",
    "We are minimizing the function:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 \\sin(x)\n",
    "$$\n",
    "\n",
    "such that the value of $x \\in [1, 8]$.\n",
    "\n",
    "\n",
    "\n",
    "# The Standard Gradient Descent Update Rule\n",
    "\n",
    "To minimize a function, we will repeatedly apply:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\frac{df}{dx}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ x_t $ = current value  \n",
    "- $ \\eta $ = learning rate  \n",
    "- $ \\frac{df}{dx} $ = derivative at $ x_t $\n",
    "\n",
    "We subtract the derivative because it points in the direction of steepest increase — we want to go in the opposite direction.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.  **Define the Function (`f(x)`):** * We define our target function  `def f(x): return x**2 * torch.sin(x)`. PyTorch will automatically build a computation graph for these operations.\n",
    "\n",
    "2.  **Initialize `x` (`torch.tensor` with `requires_grad=True`):** Start with an initial guess for `x = 2.3` and specify the device and that `requires_grad=True` (*very important* PyTorch flag) \n",
    "\n",
    "3.  **Optimization Parameters:**\n",
    "    *   `learning_rate`: This controls the step size taken in the direction of the negative gradient. A large learning rate can overshoot the minimum, while a small one can make convergence very slow (TRY IT!). Finding a good learning rate is often an experimental process.\n",
    "    *   `num_iterations`: The number of times we repeat the optimization step. The more iterations, the more chances `x` has to move towards a minimum.\n",
    "\n",
    "4.  **The Gradient Descent Loop:**\n",
    "    This is the core of the optimization:\n",
    "\n",
    "    *   **Forward Pass (`y = f(x)`):**\n",
    "        *   `y = f(x)`: We compute the value of the function `f(x)` with the current `x`. During this step, PyTorch records the operations in the computation graph.\n",
    "\n",
    "    *   **Backward Pass (`y.backward()`):**\n",
    "        *   `y.backward()`: This is where the magic of **automatic differentiation** happens! PyTorch traverses the computation graph backward from `y` to `x`, computing the gradient of `y` with respect to `x` (`dy/dx`). These gradients are then stored in `x.grad`.\n",
    "\n",
    "    *   **Update `x` (`with torch.no_grad(): x -= learning_rate * x.grad`):**\n",
    "        *   `with torch.no_grad()`: This block is crucial. When updating `x` using its gradient, we don't want PyTorch to track these update operations as part of the computation graph for *future* gradient calculations. Disabling gradient tracking saves memory and computation.\n",
    "        *   `x -= learning_rate * x.grad`: This is the actual gradient descent update rule. We move `x` in the opposite direction of the gradient, scaled by the `learning_rate`. This step takes `x` towards a lower value of `f(x)`.\n",
    "\n",
    "    *   **Clamping `x` (`x.clamp_(1, 8)`):**\n",
    "        *   `x.clamp_(1, 8)`: This ensures that our `x` value stays within the specified range of 1 to 8. `clamp_` is an in-place operation that modifies `x` directly.\n",
    "        \n",
    "    *   **Zero Gradients (`x.grad.zero_()`):**\n",
    "        *   `if x.grad is not None: x.grad.zero_()`: Before computing new gradients in each iteration, we must explicitly set the gradients of `x` to zero. PyTorch *accumulates* gradients by default. If we don't zero them, the new gradients would be added to the old ones, leading to incorrect updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the function\n",
    "def f(x):\n",
    "    return x**2 * torch.sin(x)\n",
    "\n",
    "# 2. Initialize x within the range [1, 8]\n",
    "x = torch.tensor([2.3], device=device, requires_grad=True)\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "# 3. Gradient Descent loop\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Iteration {i+1:03d}: x = {x.item():.4f}, f(x) = {f(x).item():.4f}\")\n",
    "\n",
    "    # Forward pass\n",
    "    y = f(x)\n",
    "\n",
    "    # Backward pass (compute gradients)\n",
    "    y.backward()\n",
    "\n",
    "    # Update x using gradient descent\n",
    "    with torch.no_grad(): # Disable gradient tracking for the update step\n",
    "\n",
    "        # Update gradients\n",
    "        x -= learning_rate * x.grad\n",
    "\n",
    "        # Keep x within bounds [1, 8]\n",
    "        x.clamp_(1, 8) # In-place clamp\n",
    "    \n",
    "    # Zero gradients\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(f\"\\nFinal optimized x: {x.item():.4f}, f(x): {f(x).item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fb5b4",
   "metadata": {},
   "source": [
    "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images/sgd_bad.gif\" style=\"width:400;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1d765",
   "metadata": {
    "id": "b9c1d765"
   },
   "source": [
    "\n",
    "## 3) Apply the basics: MLP classification on non-linearly separable data (two moons)\n",
    "\n",
    "We now use the mechanics from sections (1) and (2) to:\n",
    "- generate a **two-moons** dataset (not linearly separable)\n",
    "- train a small MLP with `CrossEntropyLoss`\n",
    "- visualize the decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc37358",
   "metadata": {
    "id": "9cc37358"
   },
   "source": [
    "### 3.1 Generate two-moons data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c335725",
   "metadata": {},
   "source": [
    "In the context of the `make_moons` function used in this notebook:\n",
    "\n",
    "*   **Features (X)**: This is a tensor of shape `(n_samples, 2)`, where `n_samples` is the total number of data points. Each row `[x1, x2]` represents the 2D coordinates of a single data point. These are the inputs your MLP model will use to make predictions.\n",
    "\n",
    "*   **Labels (y)**: This is a tensor of shape `(n_samples,)`, containing the corresponding class labels for each data point. The labels are either `0` or `1`, indicating which of the two 'moons' a particular point belongs to. These are the target values that your model aims to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9ca83",
   "metadata": {
    "id": "e7a9ca83"
   },
   "outputs": [],
   "source": [
    "def make_moons(n_samples=2000, noise=0.18, distance=0.30, device=\"cpu\"):\n",
    "    n0 = n_samples // 2\n",
    "    n1 = n_samples - n0\n",
    "\n",
    "    theta0 = torch.rand(n0, device=device) * math.pi\n",
    "    theta1 = torch.rand(n1, device=device) * math.pi\n",
    "\n",
    "    x0 = torch.stack([torch.cos(theta0), torch.sin(theta0)], dim=1)\n",
    "    x1 = torch.stack([1.0 - torch.cos(theta1), -torch.sin(theta1) - distance], dim=1)\n",
    "\n",
    "    X = torch.cat([x0, x1], dim=0).float()\n",
    "    y = torch.cat([torch.zeros(n0, device=device, dtype=torch.long),\n",
    "                   torch.ones(n1, device=device, dtype=torch.long)], dim=0)\n",
    "\n",
    "    X = X + noise * torch.randn_like(X)\n",
    "    perm = torch.randperm(X.size(0), device=device)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def plot_points(X, y, title=\"data\"):\n",
    "    Xc = X.detach().cpu()\n",
    "    yc = y.detach().cpu()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "X, y = make_moons(device=device)\n",
    "\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"classes:\", y.min().item(), \"to\", y.max().item())\n",
    "\n",
    "plot_points(X, y, \"Two moons (non-linearly separable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d557c7b",
   "metadata": {
    "id": "0d557c7b"
   },
   "source": [
    "### 3.2 Train/val split + DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fcc9d",
   "metadata": {},
   "source": [
    "- **`TensorDataset`**: This class wraps tensors, creating a \"dataset\" from them. Each sample in the dataset is a tuple of tensors (e.g., `(features, labels)`).\n",
    "\n",
    "  *Purpose*: It allows you to easily combine features and labels into a single dataset object.\n",
    "\n",
    "  *Usage Example*:\n",
    "  ```python\n",
    "  from torch.utils.data import TensorDataset\n",
    "  features = torch.randn(100, 10) # 100 samples, 10 features\n",
    "  labels = torch.randint(0, 2, (100,))\n",
    "  dataset = TensorDataset(features, labels)\n",
    "  ```\n",
    "\n",
    "- **`DataLoader`**: This class iterates over a `Dataset` (like `TensorDataset`), providing data in mini-batches.\n",
    "\n",
    "  *Purpose*: It simplifies batching, shuffling, and loading data in parallel (if `num_workers > 0`), which is essential for efficient model training.\n",
    "\n",
    "  *Usage Example*:\n",
    "  ```python\n",
    "  from torch.utils.data import DataLoader\n",
    "  # ... (after creating dataset as above)\n",
    "  data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "  \n",
    "  for batch_features, batch_labels in data_loader:\n",
    "      # training step with batch_features and batch_labels\n",
    "      pass\n",
    "  ```\n",
    "\n",
    "  *Arguments*:\n",
    "    - `batch_size`: Specifies the number of samples to load per batch. For example, `batch_size=32` means the `DataLoader` will yield tensors containing 32 samples.\n",
    "    - `shuffle`: If `True`, the data will be re-shuffled at every epoch. This is important for preventing the model from memorizing the order of training examples and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8056b",
   "metadata": {
    "id": "07d8056b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_val_split(X, y, val_frac=0.2):\n",
    "    N = X.size(0)\n",
    "    n_val = int(val_frac * N)\n",
    "    X_val, y_val = X[:n_val], y[:n_val]\n",
    "    X_train, y_train = X[n_val:], y[n_val:]\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "X_train, y_train, X_val, y_val = train_val_split(X, y, val_frac=0.2)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355cf50",
   "metadata": {
    "id": "b355cf50"
   },
   "source": [
    "### 3.3 Optional baseline: linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a4c86",
   "metadata": {},
   "source": [
    "#### 1. `torch.nn.Linear` (Linear Layer / Fully Connected Layer)\n",
    "\n",
    "`nn.Linear` applies a linear transformation to the incoming data.\n",
    "Given an input vector $x \\in \\mathbb{R}^{\\text{in_features}}$ and an output vector $y \\in \\mathbb{R}^{\\text{out_features}}$:\n",
    "$$y = xW^T + b$$\n",
    "Where:\n",
    "- $x$ is the input tensor of shape `(batch_size, in_features)`\n",
    "- $W$ is the weight matrix of shape `(out_features, in_features)` (its transpose $W^T$ has shape `(in_features, out_features)` for matrix multiplication with $x$)\n",
    "- $b$ is the bias vector of shape `(out_features)`\n",
    "\n",
    "\n",
    "#### 2. `torch.nn.CrossEntropyLoss` (Loss Function)\n",
    "\n",
    "**Purpose**: `nn.CrossEntropyLoss` is a commonly used loss function for multi-class classification problems. Given input logits $z = [z_0, z_1, \\dots, z_{K-1}]$ for $K$ classes and a true class label $y_{\\text{true}}$:\n",
    "\n",
    "First, apply softmax to convert logits to probabilities:\n",
    "$$p_i = \\frac{e^{z_i}}{\\sum_{j=0}^{K-1} e^{z_j}}$$\n",
    "\n",
    "Then, the Cross-Entropy Loss for a single example is:\n",
    "$$L = -\\log(p_{y_{\\text{true}}})$$\n",
    "\n",
    "For a batch of $N$ samples, the average loss is typically calculated:\n",
    "$$L = -\\frac{1}{N} \\sum_{n=1}^{N} \\log(p_{n, y_{\\text{true}_n}})$$\n",
    "\n",
    "#### 3. `torch.optim.SGD` (Stochastic Gradient Descent Optimizer)\n",
    "\n",
    "`torch.optim.SGD` implements the Stochastic Gradient Descent algorithm (or its variants) to update the parameters of a model to minimize a loss function. For each parameter $\\theta$ in the model, the update rule is:\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$\n",
    "Where:\n",
    "- $\\theta_t$ is the parameter at time step $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(\\theta_t)$ is the gradient of the loss function $L$ with respect to the parameter $\\theta$ at time $t$\n",
    "\n",
    "**How it works**:\n",
    "- It takes the model's parameters (tensors that `requires_grad=True`) and a learning rate as input.\n",
    "- In each optimization step, it uses the gradients computed during the backward pass (via `.backward()`) to adjust the parameters.\n",
    "\n",
    "**Key methods**:\n",
    "- `optimizer.zero_grad()`: Zeros out the gradients of all parameters being optimized. (Gradients accumulate by default).\n",
    "- `optimizer.step()`: Performs a single optimization step (parameter update).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69869347",
   "metadata": {
    "id": "69869347"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(2, 2)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    return (logits.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "baseline = LinearBaseline().to(device)\n",
    "opt = torch.optim.SGD(baseline.parameters(), lr=0.1)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(60):\n",
    "    baseline.train()\n",
    "    for xb, yb in train_loader:\n",
    "        logits = baseline(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "baseline.eval()\n",
    "with torch.no_grad():\n",
    "    accs = [accuracy_from_logits(baseline(xb), yb).item() for xb, yb in val_loader]\n",
    "print(\"Linear baseline val acc:\", sum(accs)/len(accs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2046796",
   "metadata": {
    "id": "c2046796"
   },
   "source": [
    "### 3.4 MLP classifier (`nn.Module`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f669f",
   "metadata": {
    "id": "714f669f"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=(64, 64), num_classes=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "model = MLP(hidden=(64, 64), dropout=0.2).to(device)\n",
    "print(\"num params:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53437b6",
   "metadata": {
    "id": "c53437b6"
   },
   "source": [
    "### 3.5 Train the MLP (end-to-end autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f53347",
   "metadata": {
    "id": "38f53347"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "train_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = running / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accs = [accuracy_from_logits(model(xb), yb).item() for xb, yb in val_loader]\n",
    "    val_acc = sum(accs) / len(accs)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"epoch {epoch+1:02d} | train loss {train_loss:.4f} | val acc {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93237c35",
   "metadata": {
    "id": "93237c35"
   },
   "source": [
    "### 3.6 Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15055935",
   "metadata": {
    "id": "15055935"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(val_accs)\n",
    "plt.title(\"Validation accuracy\"); plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892cba3",
   "metadata": {
    "id": "3892cba3"
   },
   "source": [
    "### 3.7 Decision boundary visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b3c12",
   "metadata": {
    "id": "440b3c12"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_boundary(model, X, y, grid_steps=300, pad=0.6, title=\"decision boundary\"):\n",
    "    model.eval()\n",
    "    Xc = X.detach().cpu()\n",
    "    yc = y.detach().cpu()\n",
    "\n",
    "    x_min, x_max = Xc[:, 0].min() - pad, Xc[:, 0].max() + pad\n",
    "    y_min, y_max = Xc[:, 1].min() - pad, Xc[:, 1].max() + pad\n",
    "\n",
    "    xs = torch.linspace(x_min, x_max, grid_steps)\n",
    "    ys = torch.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = torch.meshgrid(xs, ys, indexing=\"xy\")\n",
    "    grid = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=-1).to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid)\n",
    "        preds = logits.argmax(dim=-1).reshape(grid_steps, grid_steps).cpu()\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, preds, alpha=0.35)\n",
    "    plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X_val, y_val, title=\"MLP decision boundary (val set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f2281",
   "metadata": {
    "id": "078f2281"
   },
   "source": [
    "\n",
    "## 4) Homework / Extensions\n",
    "\n",
    "1. Increase noise in `make_moons(noise=0.30)` and observe accuracy + boundary.\n",
    "2. Try smaller MLPs `(8,8)` vs larger `(128,128,128)` and compare under/overfitting.\n",
    "3. Add dropout (`dropout=0.2`) and see how it affects boundary smoothness.\n",
    "4. Replace ReLU with GELU.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
