{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9252e6",
   "metadata": {
    "id": "ac9252e6"
   },
   "source": [
    "\n",
    "# PyTorch Fundamentals (Torch Tutorial Style): **Tensors** + **Autograd** + MLP on Nonlinear Data\n",
    "\n",
    "This notebook mirrors the structure of the **official PyTorch “Basics” tutorials** and then applies the concepts in a small end‑to‑end example:\n",
    "1. **Tensors**\n",
    "2. **Automatic Differentiation** with `torch.autograd`\n",
    "3. MLP classifier on **non-linearly separable** synthetic data (two moons)\n",
    "\n",
    "> Goal: make you fluent in the *core* PyTorch mechanics you’ll reuse later for larger models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368796",
   "metadata": {
    "id": "55368796"
   },
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579dba9",
   "metadata": {
    "id": "1579dba9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"device:\", device)\n",
    "\n",
    "assert torch.cuda.is_available(), \"⚠️ Please enable GPU in Runtime -> Change runtime type\"\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8cb6e",
   "metadata": {
    "id": "9fd8cb6e"
   },
   "source": [
    "\n",
    "## 1) Tensors\n",
    "\n",
    "### Informal idea\n",
    "A **tensor** is PyTorch’s core data structure: a typed, shape-aware, device-aware multi-dimensional array.\n",
    "Most deep learning is just tensor ops + automatic differentiation.\n",
    "\n",
    "### What you should walk away with\n",
    "- how to **create** tensors\n",
    "- how to inspect **shape / dtype / device**\n",
    "- basic **indexing / slicing**\n",
    "- **operations** and in-place semantics\n",
    "- moving data across **CPU/GPU**\n",
    "- interoperability with **NumPy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a7159",
   "metadata": {
    "id": "1c5a7159"
   },
   "source": [
    "### 1.1 Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fefad2",
   "metadata": {
    "id": "e7fefad2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# From Python data\n",
    "a = [[1, 2], [3, 4]]\n",
    "t1 = torch.tensor(a)\n",
    "print(\"t1:\", t1, \"| dtype:\", t1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960rut1A_KiW",
   "metadata": {
    "id": "960rut1A_KiW"
   },
   "outputs": [],
   "source": [
    "# From another tensor (shares metadata rules)\n",
    "t2 = torch.ones_like(t1)\n",
    "t3 = torch.rand_like(t1, dtype=torch.float32)\n",
    "print(\"t2:\", t2, \"| t3:\", t3, \"| t3 dtype:\", t3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YpnAusrL_LEF",
   "metadata": {
    "id": "YpnAusrL_LEF"
   },
   "outputs": [],
   "source": [
    "# Random / constant tensors\n",
    "t4 = torch.randn(3, 4)          # standard normal\n",
    "t5 = torch.zeros(2, 3, 5)       # zeros\n",
    "t6 = torch.arange(0, 10)        # 0..9\n",
    "print(\"t4 shape:\", t4.shape, \"t5 shape:\", t5.shape, \"t6:\", t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13ed19",
   "metadata": {
    "id": "6a13ed19"
   },
   "source": [
    "### 1.2 Tensor attributes: shape, dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afad74",
   "metadata": {
    "id": "d0afad74"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(2, 3, device=device, dtype=torch.float32)\n",
    "print(\"shape:\", x.shape)\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"device:\", x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d694b29",
   "metadata": {
    "id": "7d694b29"
   },
   "source": [
    "### 1.3 Indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473e142",
   "metadata": {
    "id": "3473e142"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.arange(0, 12).reshape(3, 4)\n",
    "print(x)\n",
    "\n",
    "print(\"x[0]:\", x[0])\n",
    "print(\"x[:, 1:3]:\", x[:, 1:3])\n",
    "print(\"last column:\", x[:, -1])\n",
    "\n",
    "# boolean masking\n",
    "mask = (x % 2 == 0)\n",
    "print(\"mask:\", mask)\n",
    "print(\"even elements:\", x[mask])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcafa48",
   "metadata": {
    "id": "bfcafa48"
   },
   "source": [
    "### 1.4 Basic operations (and broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a4e3",
   "metadata": {
    "id": "9706a4e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "a = torch.randn(2, 3, device=device)\n",
    "b = torch.randn(2, 3, device=device)\n",
    "\n",
    "print(\"add:\", (a + b).shape)\n",
    "print(\"mul:\", (a * b).shape)\n",
    "\n",
    "# matrix multiply\n",
    "m1 = torch.randn(2, 4, device=device)\n",
    "m2 = torch.randn(4, 3, device=device)\n",
    "mm = m1 @ m2\n",
    "print(\"matmul shape:\", mm.shape)\n",
    "\n",
    "# broadcasting: (2,3) + (3,) -> (2,3)\n",
    "v = torch.randn(3, device=device)\n",
    "c = a + v\n",
    "print(\"broadcasted add shape:\", c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad1ffe",
   "metadata": {
    "id": "64ad1ffe"
   },
   "source": [
    "### 1.5 In-place operations (use with care)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca66f6",
   "metadata": {
    "id": "f0ca66f6"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.ones(3, device=device)\n",
    "print(\"before:\", x)\n",
    "\n",
    "# in-place add\n",
    "x.add_(2)\n",
    "print(\"after add_:\", x)\n",
    "\n",
    "# In-place ops can break autograd in some situations; avoid them unless you know why you need them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e41113",
   "metadata": {
    "id": "f7e41113"
   },
   "source": [
    "### 1.6 Moving tensors across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b325c",
   "metadata": {
    "id": "799b325c"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_cpu = torch.randn(2, 2, device=\"cpu\")\n",
    "x_dev = x_cpu.to(device)\n",
    "print(\"cpu device:\", x_cpu.device, \"| moved device:\", x_dev.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc38c81",
   "metadata": {
    "id": "6dc38c81"
   },
   "source": [
    "### 1.7 NumPy bridge (CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198069c",
   "metadata": {
    "id": "2198069c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "x = torch.randn(3, 3)          # on CPU\n",
    "x_np = x.numpy()               # shares memory\n",
    "x_np[0, 0] = 123.0\n",
    "print(\"torch sees numpy change:\", x[0,0].item())\n",
    "\n",
    "y_np = np.ones((2,2), dtype=np.float32)\n",
    "y = torch.from_numpy(y_np)     # shares memory\n",
    "y[0,0] = 7.0\n",
    "print(\"numpy sees torch change:\", y_np[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04d9d3",
   "metadata": {
    "id": "be04d9d3"
   },
   "source": [
    "\n",
    "## 2) Automatic Differentiation with `torch.autograd`\n",
    "\n",
    "### Informal idea\n",
    "PyTorch can automatically compute gradients for tensor expressions by building a **dynamic computation graph** during the forward pass.\n",
    "\n",
    "### Formal idea (very compact)\n",
    "For a scalar objective \\(L(\theta)\\), PyTorch computes \\(\n",
    "abla_\theta L\\) via reverse‑mode AD (backprop).\n",
    "\n",
    "### What you should walk away with\n",
    "- how `requires_grad` controls tracking\n",
    "- what `.backward()` does\n",
    "- how gradients accumulate in `.grad`\n",
    "- when to use `detach()` and `torch.no_grad()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1c2b",
   "metadata": {
    "id": "9cbe1c2b"
   },
   "source": [
    "### 2.1 A tiny scalar example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41dd84",
   "metadata": {
    "id": "db41dd84"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(2.0, device=device, requires_grad=True)\n",
    "y = x**3 + 2*x + 1  # y = x^3 + 2x + 1\n",
    "y.backward()        # dy/dx\n",
    "\n",
    "print(\"x:\", x.item())\n",
    "print(\"y:\", y.item())\n",
    "print(\"dy/dx should be 3x^2 + 2 =\", 3*(2.0**2) + 2)\n",
    "print(\"autograd dy/dx:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1db793",
   "metadata": {
    "id": "4d1db793"
   },
   "source": [
    "### 2.2 Vector gradients: scalar output required for `.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb711bc",
   "metadata": {
    "id": "6eb711bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(3, device=device, requires_grad=True)\n",
    "y = (x * x).sum()  # scalar\n",
    "y.backward()\n",
    "print(\"x:\", x)\n",
    "print(\"grad (should be 2x):\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea988ea",
   "metadata": {
    "id": "9ea988ea"
   },
   "source": [
    "### 2.3 Gradient accumulation (why you zero grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c412b",
   "metadata": {
    "id": "ff5c412b"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(1.0, device=device, requires_grad=True)\n",
    "\n",
    "y1 = x * 3\n",
    "y1.backward()\n",
    "print(\"after first backward:\", x.grad.item())  # 3\n",
    "\n",
    "y2 = x * 4\n",
    "y2.backward()\n",
    "print(\"after second backward (accumulated):\", x.grad.item())  # 3 + 4\n",
    "\n",
    "# reset\n",
    "x.grad.zero_()\n",
    "print(\"after zero_:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af127a90",
   "metadata": {
    "id": "af127a90"
   },
   "source": [
    "### 2.4 Detaching and `no_grad` (stop tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c91de4",
   "metadata": {
    "id": "70c91de4"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(2, 2, device=device, requires_grad=True)\n",
    "y = (x @ x).sum()\n",
    "\n",
    "# detach: new tensor shares storage but has no grad history\n",
    "x_det = x.detach()\n",
    "print(\"x_det requires_grad?\", x_det.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x * 10\n",
    "print(\"z requires_grad?\", z.requires_grad)\n",
    "\n",
    "# You typically use no_grad during evaluation/inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feced132",
   "metadata": {
    "id": "feced132"
   },
   "source": [
    "### 2.5 Inspecting the computation graph (light touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010af66",
   "metadata": {
    "id": "9010af66"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(3, device=device, requires_grad=True)\n",
    "y = (x.sin() * x).sum()\n",
    "print(\"y.grad_fn:\", y.grad_fn)  # shows last operation node\n",
    "y.backward()\n",
    "print(\"x.grad:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1d765",
   "metadata": {
    "id": "b9c1d765"
   },
   "source": [
    "\n",
    "## 3) Apply the basics: MLP classification on non-linearly separable data (two moons)\n",
    "\n",
    "We now use the mechanics from sections (1) and (2) to:\n",
    "- generate a **two-moons** dataset (not linearly separable)\n",
    "- train a small MLP with `CrossEntropyLoss`\n",
    "- visualize the decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc37358",
   "metadata": {
    "id": "9cc37358"
   },
   "source": [
    "### 3.1 Generate two-moons data (no sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9ca83",
   "metadata": {
    "id": "e7a9ca83"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_moons(n_samples=2000, noise=0.18, distance=0.30, device=\"cpu\"):\n",
    "    n0 = n_samples // 2\n",
    "    n1 = n_samples - n0\n",
    "\n",
    "    theta0 = torch.rand(n0, device=device) * math.pi\n",
    "    theta1 = torch.rand(n1, device=device) * math.pi\n",
    "\n",
    "    x0 = torch.stack([torch.cos(theta0), torch.sin(theta0)], dim=1)\n",
    "    x1 = torch.stack([1.0 - torch.cos(theta1), -torch.sin(theta1) - distance], dim=1)\n",
    "\n",
    "    X = torch.cat([x0, x1], dim=0).float()\n",
    "    y = torch.cat([torch.zeros(n0, device=device, dtype=torch.long),\n",
    "                   torch.ones(n1, device=device, dtype=torch.long)], dim=0)\n",
    "\n",
    "    X = X + noise * torch.randn_like(X)\n",
    "    perm = torch.randperm(X.size(0), device=device)\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "X, y = make_moons(device=device)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"classes:\", y.min().item(), \"to\", y.max().item())\n",
    "\n",
    "def plot_points(X, y, title=\"data\"):\n",
    "    Xc = X.detach().cpu()\n",
    "    yc = y.detach().cpu()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_points(X, y, \"Two moons (non-linearly separable)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d557c7b",
   "metadata": {
    "id": "0d557c7b"
   },
   "source": [
    "### 3.2 Train/val split + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8056b",
   "metadata": {
    "id": "07d8056b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_val_split(X, y, val_frac=0.2):\n",
    "    N = X.size(0)\n",
    "    n_val = int(val_frac * N)\n",
    "    X_val, y_val = X[:n_val], y[:n_val]\n",
    "    X_train, y_train = X[n_val:], y[n_val:]\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "X_train, y_train, X_val, y_val = train_val_split(X, y, val_frac=0.2)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"train:\", X_train.shape, \"val:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355cf50",
   "metadata": {
    "id": "b355cf50"
   },
   "source": [
    "### 3.3 Optional baseline: linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69869347",
   "metadata": {
    "id": "69869347"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(2, 2)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    return (logits.argmax(dim=-1) == y).float().mean()\n",
    "\n",
    "baseline = LinearBaseline().to(device)\n",
    "opt = torch.optim.SGD(baseline.parameters(), lr=0.1)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(60):\n",
    "    baseline.train()\n",
    "    for xb, yb in train_loader:\n",
    "        logits = baseline(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "baseline.eval()\n",
    "with torch.no_grad():\n",
    "    accs = [accuracy_from_logits(baseline(xb), yb).item() for xb, yb in val_loader]\n",
    "print(\"Linear baseline val acc:\", sum(accs)/len(accs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2046796",
   "metadata": {
    "id": "c2046796"
   },
   "source": [
    "### 3.4 MLP classifier (`nn.Module`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f669f",
   "metadata": {
    "id": "714f669f"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=(64, 64), num_classes=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "model = MLP(hidden=(64, 64), dropout=0.2).to(device)\n",
    "print(\"num params:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53437b6",
   "metadata": {
    "id": "c53437b6"
   },
   "source": [
    "### 3.5 Train the MLP (end-to-end autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f53347",
   "metadata": {
    "id": "38f53347"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "train_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = running / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accs = [accuracy_from_logits(model(xb), yb).item() for xb, yb in val_loader]\n",
    "    val_acc = sum(accs) / len(accs)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"epoch {epoch+1:02d} | train loss {train_loss:.4f} | val acc {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93237c35",
   "metadata": {
    "id": "93237c35"
   },
   "source": [
    "### 3.6 Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15055935",
   "metadata": {
    "id": "15055935"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(val_accs)\n",
    "plt.title(\"Validation accuracy\"); plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892cba3",
   "metadata": {
    "id": "3892cba3"
   },
   "source": [
    "### 3.7 Decision boundary visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b3c12",
   "metadata": {
    "id": "440b3c12"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_boundary(model, X, y, grid_steps=300, pad=0.6, title=\"decision boundary\"):\n",
    "    model.eval()\n",
    "    Xc = X.detach().cpu()\n",
    "    yc = y.detach().cpu()\n",
    "\n",
    "    x_min, x_max = Xc[:, 0].min() - pad, Xc[:, 0].max() + pad\n",
    "    y_min, y_max = Xc[:, 1].min() - pad, Xc[:, 1].max() + pad\n",
    "\n",
    "    xs = torch.linspace(x_min, x_max, grid_steps)\n",
    "    ys = torch.linspace(y_min, y_max, grid_steps)\n",
    "    xx, yy = torch.meshgrid(xs, ys, indexing=\"xy\")\n",
    "    grid = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=-1).to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(grid)\n",
    "        preds = logits.argmax(dim=-1).reshape(grid_steps, grid_steps).cpu()\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, preds, alpha=0.35)\n",
    "    plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X_val, y_val, title=\"MLP decision boundary (val set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f2281",
   "metadata": {
    "id": "078f2281"
   },
   "source": [
    "\n",
    "## 4) Homework / Extensions\n",
    "\n",
    "1. Increase noise in `make_moons(noise=0.30)` and observe accuracy + boundary.\n",
    "2. Try smaller MLPs `(8,8)` vs larger `(128,128,128)` and compare under/overfitting.\n",
    "3. Add dropout (`dropout=0.2`) and see how it affects boundary smoothness.\n",
    "4. Replace ReLU with GELU.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
